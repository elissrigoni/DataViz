{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a82afa",
   "metadata": {},
   "source": [
    "**3. [0-5 points] Describe the PCA algorithm, highlighting its pros and cons; find a case study where PCA is more appropriate than MDS.**\n",
    "\n",
    "PRINCIPAL COMPONENT ANALYSIS (PCA) is the first algorithm that have been created. This algorithm projects orthogonally a dataset $X={x1,…,xn}$ of n p-dimensional points into a r-dimensional space with $r=min(n-1,p)$, so that in the new coordinates the projected points’ variables are uncorrelated. The idea is to decorrelate the original elements of a dataset, by decorrelating original elements, we can extract the single properties of each of them and after that it is possible to isolate the points that contain more information. \n",
    "The new coordinates (coordinates of the projection points) are called principal components, and each component is defined by the rules:\n",
    "\n",
    "• being orthogonal to the previous components: all the new points have to be perpendicular\n",
    "• having highest possible variance: all new points have to maximize the variance.\n",
    "\n",
    "The novel coordinates form an orthogonal basis pca, that geometrically can be interpreted as fitting a p-dimensional ellipsoid to the data; each axis of the ellipsoid is a principal component (geometrical interpretation of PCA).\n",
    "Starting from a dataset, the workflow of the PCA is:\n",
    "\t1. **Center data**: as first thing we center the variables to standardize the range of the continuous variables so that each of them contribute equally to the analysis. A common way to do that is by computing Z-score:  $z = \\frac{data - mean}{standard deviation}$, in this way all the data will have mean zero and standard deviation one. We obtain a matrix X in Rnxp that is the data matrix columnwise zero center. Scaling data is very important, because PCA depends on the chosen scaling.\n",
    "\t2. **Compute covariance matrix**: it represents the covariance measurement of each sample in the data set with every other sample. In general, the covariance value is computed between two random variables and represents the variables relationship with one another. If the covariance is positive, then the two variables increase together and decrease together. If the covariance is negative the variables are inversely proportional. Basically the covariance matrix attempts to display the relationships between each pair of samples in the dataset.\n",
    "\t3. **Extract eigen-values/-vectors**: The first principal component corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line (definition of variance). To find this direction, we need the eigenvalues and the eigenvector. Each eigenvalue is proportional to the portion of the \"variance\" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean, useful measure is the explained variance of k-th PCA: $\\frac{λk}{∑ λi}$ (eigenvalue associated to the k direction divided by the sum of all possible directions).\n",
    "\t4. **Normalize eigenvectors because we want orthonormal basis**: eigenvalues will be used to form the basis vectors. The larger the magnitude of the eigenvalue, the more variance its corresponding basis vector will contain. Since we are reducing dimensionality we are inevitably losing some information, however by picking basis vectors based on eigenvalues with the highest variance, we are preserving as much of the original information as possible. From the previous step we have have the eigenvectors, that will be assembled into an eigenvector matrix. To compute the basis vectors, we multiply the mean adjusted matrix computed in step 1 by the eigenvector matrix.\n",
    "\t5. **Diagonalize covariance matrix with variance on diagonal and compute proportions of variances**: we will end up with a space with all the variances explicitly computed and orthonormal basis. PCA essentially rotates the set of points around their mean in order to align them with the principal components. This moves as much variance as possible (using an orthogonal transformation) into the first few dimensions. So the top k dimensions are those that essentially include all the information of the dataset in term of explained variance.\n",
    "\n",
    "Three methods are used to compute the PCA in practice:\n",
    "1. Computing the covariance matrix is better when data have similar scales (range of the components)\n",
    "2. Computing the correlation matrix is better when data have different scales\n",
    "3. Singular value decomposition is used in more general purpose\n",
    "\n",
    "**PROS**:\n",
    "\n",
    "* It is good at extracting signal from noise: PCA is computed by finding the components which explain the greatest amount of variance, it captures the signal in the data and omits the noise.\n",
    "* Computationally inexpensive (time wise): PCA is based on linear algebra, which is computationally easy to solve by computers.\n",
    "* Good for linear data: \n",
    "\n",
    "**CONS**:\n",
    "\n",
    "* Not appropriate for non-linear data: PCA assumes a linear relationship between features, hence the algorithm is not well suited to capturing non-linear relationships. T\n",
    "* Variables are less interpretable after transformation:\n",
    "* Data standardization is required: PCA will be extremely biased towards the first feature being the first principle component, regardless of the actual maximum variance within the data. This is why it’s so important to standardize the values first.\n",
    "* Information loss, even if PCA tries to maximize the variance, the number of PCs has to be selected with care. Although dimensionality reduction is useful, it comes at a cost: information loss is a necessary part of PCA. Balancing the trade-off between dimensionality reduction and information loss is unfortunately a necessary compromise that we have to make when using PCA.\n",
    "\n",
    "**Case study where PCA is more appropriate than MDS**\n",
    "Principal Component Analysis performs well in identifying all influencing factors affecting results in individual areas. Also correlating factors associated with candidate win/lose. Not only in the election commission, the PCA technique is used in many applications and different industries and multiple areas and fields.\n",
    "\n",
    "It was used in a study that aims at exploring segments of consumer loyalty and its influential factors towards safe food brands in the country (\"*Exploring consumer loyalty towards brands of safe vegetables in Vietnam*\").In this study, PCA is used for reducing variables and simplifying the interpretations of the components formed from items related to psychological variables, including loyalty, brand trust, brand familiarity and brand satisfaction. It was considered the method preferred, due to multidimensional constructs of behavioural and attitudinal loyalty.The results of PCA in confirmed two factors, namely, Factor 1 (attitudinal loyalty) and Factor 2 (behavioural loyalty) with KMO = 0.782. All items to measure two dimensions of brand loyalty were satisfactory to be retained after PCA. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
