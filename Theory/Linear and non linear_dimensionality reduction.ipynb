{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1927d258",
   "metadata": {},
   "source": [
    "**3. [0-5 points] Describe the differences between linear and non-linear algorithms in dimensionality reduction, and provide an example highlighting such differences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3ca7e",
   "metadata": {},
   "source": [
    "The concept of dimensionality reduction consists in finding a function that push my points that lead in a D space to the smallest possible space where all these things are conserved. Due to the low intrinsic dimension of data, we can reduce the (extrinsic) dimension without losing much information for many types of real-life high-dimensional data, avoiding many of the curses of dimensionality. Dimensionality reduction consists in a set of algorithms whose aim is finding a parameterization of the manifold which the points of data reside on.\n",
    "\n",
    "First problem: select the intrinsic dimension (real dimension where my data lead)\n",
    "A finite data set can be embedded in many manifolds of various dimensions, depending on the geometric structures defined on the data.\n",
    "\n",
    "\n",
    "* **LINEAR** = output data are a projection of the original data. For example:\n",
    "1. principal component analysis (PCA)\n",
    "2. multidimensional scaling (MDS)\n",
    "\n",
    "When implementing linear dimensionality reduction, we are trying to use as the lowest dimensional manifold a linear subspace, called hyperplane (a plane in S embedded in the larger space). The right hyperplane is found choosing, among all hyperplanes including the dataset, the one with lowest dimension. There exists a linear transformation $T$ that allows to project all points in the dataset from the original larger space onto the hyperplane found.\n",
    "\tFormally,   $T: ℝs → S\tx = T(y) = Uy + x0$,  where $U$ is an orthogonal matrix $(<u,v> = <Uu,Uv>)$ and $x0$ is the nearest point on hyperplane S to the origin. If this transformation exists, the inverse of T provides a S-dim parameterization of X (from hyperplane to original space): $Y = {y ∈ ℝs : y = T-1(x), x ∈ X}$. $U$ is an orthonormal basis of S distances are preserved and we have an example of linear dimensionality reduction.\n",
    "\n",
    "That means that there exists a linear transformation where the matrix U is orthogonal and is the nearest point on to the origin. The inverse of the map that re-send to the original  provides a dimension parametrization of the dataset without loss of generality we can always assume that and if, then is an orthonormal basis.\n",
    " \n",
    "* **NON-LINEAR** = output data are the manifold coordinate representation of the original data in the projected space. There are a lot of algorithms, some examples are:\n",
    "- isomap\n",
    "- t-sne, umap\n",
    "\n",
    "If the underlying geometry is nonlinear, the previous approach does not give a true dimension, so we have to use the topological manifold theory. \n",
    "If a manifold M in RD has dimension $s < D$ (we do not have an hyperplane), then the neighborhood of each point of $M$ is isomorphic to $Rs$. In other words there is an invertible differentiable map from $M$ to $Rs$  (where we want to project data), whose inverse if differentiable and the projection can be done in a reasonable way.\n",
    "Given a space $S$, the open covering of $X$ in $S$ is a collection $O$ of open sets in $S$ whose union contains X. A set $U$ is open if each point of $U$ has a neighborhood included in $U$. A result in geometry says that if I have a set in s-dimensional space X can be covered by open spheres, such that each point belongs to at most $s + 1$ open spheres in which it can be embedded.\n",
    "A subset $X$ of a topological space $S$ is said to have topological dimension s (also called Lebesgue covering dimension) if every covering $O$ of $X$ has a refinement $O′$ such that every point of $X$ is covered by at most $s + 1$ open sets in $O′$, and s is the smallest among such integers. \n",
    "\n",
    "In the linear case the process was much easier, since we only had to pick up among all possible hyperplanes the one living in the smallest possible dimension. In the non linear one, we have to find a way to assess each possibility.\n",
    "\n",
    "\n",
    "When chosing an approximization, we have to find a dimension d good enough to approximate all these properties, understand the structure of the dataset and save the information of the data itself.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
