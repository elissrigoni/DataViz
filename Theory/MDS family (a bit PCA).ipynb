{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6ed0c1e",
   "metadata": {},
   "source": [
    "**3. [0-5 points] Describe the MDS family of algorithms, and comment similarities and differences with PCA. Then explain the differences between Classical, Metric and Non-Metric MDS, and provide an example where only one of the three above choices is adequate.**\n",
    "\n",
    "Multidimensional scaling is a family of algorithms visualizing the level of similarity of individual cases of a dataset. In practice, MDS finds an embedding of n objects into a r-dimensional euclidean space Rn so to preserve as well as possible the distances between original points. Usually is not possible to preserve the actual distances, but only a function of them. \n",
    "\n",
    "**Difference between PCA and non-metric MSD**\n",
    "While PCA aims at describe the principal components (linear combinations) of a dataset that is able to explain the largest amount of variation, MDS, instead, aims at constructing a reduced dimensionality space so that the projected points preserve the mutual distances, inherited from the original dataset: the distance they have in the original dataset are - more or less - preserved in the projected space. When is this useful? In the case of objects of very different nature, for which there is no possible description in a feature space; examples of such cases are objects like drugs, images, trees or other complex objects without any obvious coordinates in . Despite that, a dissimilarity matrix can be constructed, that explains the mutual separation between these objects. Differently from PCA, MDS can produce some negative eigenvalues and this is due to the fact that the data do not come from a Euclidean space. MDS and PCA are probably not at the same level to be in line or opposite to each other. PCA is just a method while MDS is a class of analysis. As mapping, PCA is a particular case of MDS. On the other hand, PCA is a particular case of Factor analysis which, being a data reduction, is more than only a mapping, while MDS is only a mapping. PCA is the first algorithm that have been created. The idea is to decorrelate the original elements of a dataset, by decorrelating original elements, we can extract the single properties of each of them and after that it is possible to isolate the points that contain more information. The input to PCA is the original vectors in n-dimensional space. And the data are projected onto the directions in the data with the most variance. Hence the “spread” of the data is roughly conserved as the dimensionality decreases. On the other hand the input to MDS is the pairwise distances between points. The output of MDS is a two- or three-dimensional projection of the points where distances are preserved. MDS can be distinguished in 3 types, depending on the objective function:\n",
    "\n",
    "* **classical MDS** - also known as Pricipal Coordinates Analysis (PCoA), the objective function is called strain and involves directly the original distances between objects. The solution is deterministic and the core of the dimensionality reduction is the same as PCA (changing the objective function). If there exists a space Rp where all the original distances between objects are preserved, the distance d is called euclidean; for an euclidean distance, the classical MDS solution is unique up to isometries (a rigid transformation of the space that conserve the distance, for instance rotation, symmetry…)\n",
    "\n",
    "* **metric MDS** -  it is a superset of classical MDS, so that it generalizes the optimization procedure to other kind of loss functions. A popular function to minimize is called stress and it involves a function of the original distances. In metric mds we can find Sammon mapping that is able to preserve the small giving them a greater degree of importance in the fitting procedure that for larger . With respect to classical MDS, Sammon mapping better preserves inter-distances for smaler dissimilarities, while proportionally squeezes the inter-distances for larger dissimilarities.\n",
    "\n",
    "* **non-metric MDS** – if we are not in a metric space the original distances are assessed as dissimilarities, so the stress function finds a non-parametric monotonic relationship between the dissimilarities in the item-item matrix and the Euclidean distances between items, and defines the location of each item. A common algorithm in non-metric MDS is Kruskal mds. In other words, it is possible to say that with non-metric MDS absolute values are not considered as meaningful, only the ranking is important, so that MDS tries to find a low-dimensional representation that respects the ranking of distances. Non-metric MDS Fulfills a clear objective without many assumptions (just minimize stress), results do not change with rescaling or monotonic variable transformation, it works even if starting just from ranking information. By the way, it is slow in large problems, usually it is able to found a local (not global) optimum. Non-metric MDS is widely used when dissimilarities are known only by their rank order, and the spacing between successively ranked dissimilarities is of no interest or is unavailable. f is implicitly defined as a regression curve, and only preserves the order of distance d, that is: $f(dij)<f(dkl)$ if $dij<dkl$ thus only the order of d is needed, not the actual values. (Most common algorithm is the Kruskal MDS.)\n",
    "In metric and non-metric case, the process is given by optimization (deterministic solution is not feasible).\n",
    "\n",
    "**A case when only non-metric MdS is adequate (with respect to the others kind of MdS)**\n",
    "Non-metric MDS is iterative and non-parametric and allows one to use any distance measure that might be suitable for the data. It does not make any assumptions about a linear relationship. NMDS **arranges points to maximize rank-order correlation between real-world distance and ordination space distance**. Unlike PCA (which uses Eucliden distances), non-metric MDS relies on rank orders (distances) for ordination (i.e. non-metric).\n",
    "In the case of a dissimilarity matrix, which is not a distance matrix, classical MDS gives inconsistents results. In these cases Non-metric MDS is highly suggested over classical MDS. To have a general theoretical grip: in all the cases/experiments in which an interobserver agreement (IOA) between people, instead of actual distances, is used in the construction of the dissimilarity matrix. As practical examples I would say The Ekman colors study and the perception of similarity between different sports."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
