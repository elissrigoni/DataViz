---
title: "Text 12 June 2020"
output: html_notebook
---

**1. [0-5 points] Describe in detail the elements of visual encoding (marks and annotations), their meaning and their use. Explain and comment the meaning of the visual encoding elements in the following picture.**

```{r}
#knitr::include_graphics("/Users/elisarigoni/Documents/data_science/FirstYear/II_Semester/Data Visualization/Lab R/08_Reproduce plot/IMG/TheEconomist.gif")
```

Reading a graph is the first sample data representation, whose steps are:

1. **Dataviz perception**: consists in decoding the elements of a graph (shapes, sizes, positions, colors).
2. **Dataviz interpretation**: consists in the decoding of the meaning of a graph, i.e. making sense of the overall graphic construction.

While talking about the developing of your design solution, data representation is one of the most important layer of the visualization design structure. A visualizer (the person in charge of creating visualizations) does the reverse process of decoding, which is decoding. Decoding consists in assigning visual properties to data values. And this is the basic of any data representation, along with the components that help completing the chart display.

There are different way to encode data, anyway data encoding is always a combination of marks and attributes. The way of getting the best data representation is finding the right blend of marks & attributes that most effectively portray the angle of analysis you wish to show.

* **Marks** are visibile features (e.g., dots, line and areas). An individual mark can represent a record of instance of data, but it can also represent an aggregation of records or instances. Kind of marks:
+ *point*: the point mark has no variation (is constant) in the spatial dimension. It is largely a placeholder commonly used to represent a quantity through position on a scale, forming the basis of, for example, *scatter plots*.
+ *line*: The line mark has one (linear) spatial dimension. It is commonly used to represent quantitative value through variation in size, forming the basis of, for example, the *bar chart*.
+ *area*: The area mark has two (quadratic) spatial dimensions. It is commonly used to represent quantitative values through variation in size and position, forming the basis of, for example, *bubble plots*.
+ *form*: The form mark has three (cubic) spatial dimensions. It might be used to represent quantitative values through variation in size (specifically through volume), forming the basis of, for example, a *3D proportional shape chart*.

* **Attributes** are variations applied to the appearence of marks, such as size, position, or color. They are used to represent the values held by different quantitative or categorical variables against each record, instance or aggregation. Kind of attributes:

1. **quantitative** ones:
+ *position*: position along a scale is used to indicate a quantitative value.
+ *size*: size (length, area, volume) is used to represent quantitative values based on proportional scales where the larger the size of the mark, the larger the quantity.
+ *angle/slope*: variation in the size of angle forms the basis of pie chart sectors representing parts-of-a-whole quantitative values; the larger the angle, the larger the proportion. The slope of an incline formed by angle variation can also be used to encode values.
+ *quantity*: the quantity of a repeated set of point marks can be used to represent a one-to-one or a one-to-may unit count.
+ *color saturation*: it can be used (often in conjunction with other color properties) to represent quantitative scales, tipically the greater the saturation, the higher the quantity.
+ *color lightness*: it can be used (often in conjunction with other color properties) to represent qualitative scales, typically the larger the color, the higher the quantity.
+ *pattern*: variation in pattern density or difference in pattern texture can be used to represent quantitative scales or distinguish between categorical ordinal states.
+ *motion*: motion is more rarely seen but i could be used as a binary indicator to draw focus (motion vs no motion) or by incorporating movement through speed and direction to represent a quantitative scale ramp.

2. **Categorical** ones:
+ *symbol/shape*: symbols or shapes are generally used with points markers to indicate categorical associations.
+ *color hue*: it is typically used for distinguishing different categorical data values, but can also be used in conjunction with other color properties to represent certain quantitative scales.

3. **Relational** attributes:
+ *connection/edge*: A connection or edge indicates a relationship between two nodes. Sometimes arrows may be added to indicate direction of relationship, but largerly it is just about the presence or absence of a connection.
+ *containment*: containment is a way of indicating a grouping relationship between categories that belong to a related hierarchical parent category.


**In the specific case of the provided plot:**

This is a time series scatterplot.

The circles are point marks that represent a quantity through a position scale: in this case the scale is that of the percentage in a given moment of time for a certain politician during 2012 elections. their attributes are the position of the circle, that represent, in fact, a quantitative value in the percentage scale and a different colour hue to distinguish the two candidates (categorical attribute). So, each circle represent an opinion poll (expressed as a percentage in the y axis) from a certain source for a specific candidate in a specific moment of time (x axis).

The lines are line marks, where their variation in slope represent a certain quantitative value (percentage). Another attribute is the color hue that, paired with the one used for the circles, represent the percentage values associated to the same candidates.

The two lines portray a certain statistic based on the actual values given by the opinion polls (the dots). So it may be that each date (in the x axis) has multiple blue and red dots representing the preference percentage according to different sources and the lines are mean percentage values over that specific moment in time. So, the two lines give a summing up perception of the trends of preference for the two candidates over time.





**2. [0-5 points] Describe the step "Editorial Choices" in the Stage 3 of the Data Visualization Workflow; then provide 3 data visualization examples (not those in the lecture slides) to highlight focus, framing and angle concepts, respectively.**

STAGE 3 of the Data Workflow consists in ESTABLISHING EDITORIAL THINKING. It refers to the editorial choices: concerns the choice of which of the many viable perspectives offered by data you will focus on:

* Angle: Choosing an angle consists in selecting one or more viewpoint(s) in the analysis and in the representation of data, but also choose in which dimension you want to split all the subject. First of all, you have to ask yourself, if they are relevant or not, understand the reason why you want to choose this angle/point of view and if it is worth showing to this audience. The key is finding a duality between normal and exceptional and to not choose a viewpoint hoping for someone to find the relevance, because when representing data, YOU should be aware of the relevance. In addiction to that you have to ask yourself if they are sufficient to bring on the message, but keeping in mind that we do not need too many angles across space/time, doing an effective selection is important. 

* Framing: Framing regards filtering which data to include or to exclude (remove unnecessary clutter because the presentation would be hard to read). Balance is an essential part, but thinking about how much content the audience can process is an important step. 

* Focus: Focus is needed to emphasis what is more important and provide a visual hierarchy and the reader should be able to understand what is more important. It consists in choosing fore-/mid-/back-ground, this can be done selecting size, color and location in an appropriate way.

**3. [0-5 points] Describe the MDS family of algorithms, and comment similarities and differences with PCA. Then explain the differences between Classical, Metric and Non-Metric MDS, and provide an example where only one of the three above choices is adequate.**

Multidimensional scaling is a family of algorithms visualizing the level of similarity of individual cases of a dataset. In practice, MDS finds an embedding of n objects into a r-dimensional euclidean space Rn so to preserve as well as possible the distances between original points. Usually is not possible to preserve the actual distances, but only a function of them. 

**Difference between PCA and non-metric MSD**
While PCA aims at describe the principal components (linear combinations) of a dataset that is able to explain the largest amount of variation, MDS, instead, aims at constructing a reduced dimensionality space so that the projected points preserve the mutual distances, inherited from the original dataset: the distance they have in the original dataset are - more or less - preserved in the projected space. When is this useful? In the case of objects of very different nature, for which there is no possible description in a feature space; examples of such cases are objects like drugs, images, trees or other complex objects without any obvious coordinates in . Despite that, a dissimilarity matrix can be constructed, that explains the mutual separation between these objects. Differently from PCA, MDS can produce some negative eigenvalues and this is due to the fact that the data do not come from a Euclidean space. MDS and PCA are probably not at the same level to be in line or opposite to each other. PCA is just a method while MDS is a class of analysis. As mapping, PCA is a particular case of MDS. On the other hand, PCA is a particular case of Factor analysis which, being a data reduction, is more than only a mapping, while MDS is only a mapping. PCA is the first algorithm that have been created. The idea is to decorrelate the original elements of a dataset, by decorrelating original elements, we can extract the single properties of each of them and after that it is possible to isolate the points that contain more information. The input to PCA is the original vectors in n-dimensional space. And the data are projected onto the directions in the data with the most variance. Hence the “spread” of the data is roughly conserved as the dimensionality decreases. On the other hand the input to MDS is the pairwise distances between points. The output of MDS is a two- or three-dimensional projection of the points where distances are preserved. MDS can be distinguished in 3 types, depending on the objective function:

* **classical MDS** - also known as Pricipal Coordinates Analysis (PCoA), the objective function is called strain and involves directly the original distances between objects. The solution is deterministic and the core of the dimensionality reduction is the same as PCA (changing the objective function). If there exists a space Rp where all the original distances between objects are preserved, the distance d is called euclidean; for an euclidean distance, the classical MDS solution is unique up to isometries (a rigid transformation of the space that conserve the distance, for instance rotation, symmetry…)

* **metric MDS** -  it is a superset of classical MDS, so that it generalizes the optimization procedure to other kind of loss functions. A popular function to minimize is called stress and it involves a function of the original distances. In metric mds we can find Sammon mapping that is able to preserve the small giving them a greater degree of importance in the fitting procedure that for larger . With respect to classical MDS, Sammon mapping better preserves inter-distances for smaler dissimilarities, while proportionally squeezes the inter-distances for larger dissimilarities.

* **non-metric MDS** – if we are not in a metric space the original distances are assessed as dissimilarities, so the stress function finds a non-parametric monotonic relationship between the dissimilarities in the item-item matrix and the Euclidean distances between items, and defines the location of each item. A common algorithm in non-metric MDS is Kruskal mds. In other words, it is possibile to say that with non-metric MDS absolute values are not considered as meaningful, only the ranking is important, so that MDS tries to find a low-dimensional representation that respects the ranking of distances. Non-metric MDS Fulfills a clear objective without many assumptions (just minimize stress), results do not change with rescaling or monotonic variable transformation, it works even if starting just from ranking information. By the way, it is slow in large problems, usually it is able to found a local (not global) optimum. Non-metric MDS is widely used when dissimilarities are known only by their rank order, and the spacing between successively ranked dissimilarities is of no interest or is unavailable. f is implicitly defined as a regression curve, and only preserves the order of distance d, that is: $f(dij)<f(dkl)$ if $dij<dkl$ thus only the order of d is needed, not the actual values. (Most common algorithm is the Kruskal MDS.)
In metric and non-metric case, the process is given by optimization (deteriministic solution is not feasible).

**A case when only non-metric MdS is adequate (with respect to the others kind of MdS)**
Non-metric MDS is iterative and non-parametric and allows one to use any distance measure that might be suitable for the data. It does not make any assumptions about a linear relationship. NMDS **arranges points to maximize rank-order correlation between real-world distance and ordination space distance**. Unlike PCA (which uses Eucliden distances), non-metric MDS relies on rank orders (distances) for ordination (i.e. non-metric).
In the case of a dissimilarity matrix, which is not a distance matrix, classical MDS gives inconsistents results. In these cases Non-metric MDS is highly suggested over classical MDS. To have a general theoretical grip: in all the cases/experiments in which an interobserver agreement (IOA) between people, instead of actual distances, is used in the construction of the dissimilarity matrix. As practical examples I would say The Ekman colors study and the perception of similarity between different sports.


